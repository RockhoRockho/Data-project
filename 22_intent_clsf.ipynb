{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RockhoRockho/Data-project/blob/main/22_intent_clsf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9dodc22TAhz",
        "outputId": "174533ef-b618-4c08-c1de-fc77d20c9926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMS2Wl7-ROX8"
      },
      "source": [
        "# Data Processing \n",
        "* Intent Classification 테스크를 위한 데이터 처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDSkBnT_ROX-",
        "outputId": "eea55bbc-41e6-4097-ade2-3ac9336fca5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrCi1hW8ROX_",
        "outputId": "c45dd733-9003-449d-c35c-56164b9f3cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ],
      "source": [
        "pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jh5we_nROYA",
        "outputId": "46887112-87da-4a40-e45f-b97453d64ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n"
          ]
        }
      ],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_zEW7FfS4aV",
        "outputId": "03811951-8ce4-42ec-c3e6-a7e71f6e74ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting src\n",
            "  Using cached src-0.0.7.zip (6.3 kB)\n",
            "Building wheels for collected packages: src\n",
            "  Building wheel for src (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for src\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for src\n",
            "Failed to build src\n",
            "Installing collected packages: src\n",
            "    Running setup.py install for src ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-j3nxk4p4/src_211e5547103f4ed08da5105c3f06cab9/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-j3nxk4p4/src_211e5547103f4ed08da5105c3f06cab9/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-1kn5yldj/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/src Check the logs for full command output.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HIwxJQEROYA",
        "outputId": "7bf242f7-5d8c-4fa1-b073-b92ba96b3e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/kmkurn/pytorch-crf\n",
            "  Cloning https://github.com/kmkurn/pytorch-crf to /tmp/pip-req-build-glgj02qr\n",
            "  Running command git clone -q https://github.com/kmkurn/pytorch-crf /tmp/pip-req-build-glgj02qr\n",
            "Building wheels for collected packages: pytorch-crf\n",
            "  Building wheel for pytorch-crf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-crf: filename=pytorch_crf-0.7.2-py3-none-any.whl size=6289 sha256=1ff1ed8ff61b5e8d56d509047c603c070a235190100f1353c5b3cc18a34d7ac7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tgmwtyiy/wheels/5a/09/7f/a56999c3ea15a3adf587c4e900b4fd03c3c8a3cd49ad5b7dac\n",
            "Successfully built pytorch-crf\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.7.2\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/kmkurn/pytorch-crf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "LSRjxQs3ROYA",
        "outputId": "7c73fea9-241a-4cac-edd5-f32e189c160f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-81653216e2fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallbackAny2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEpochLogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMakeEmbed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "\n",
        "from src.dataset import Preprocessing\n",
        "from src.model import EpochLogger, MakeEmbed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZiFV-J-ROYB"
      },
      "outputs": [],
      "source": [
        "class MakeDataset:\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.intent_label_dir = \"./data/dataset/intent_label.json\"\n",
        "        self.intent_data_dir = \"./data/dataset/intent_data.csv\"\n",
        "        \n",
        "        self.intent_label = self.load_intent_label()\n",
        "        self.prep = Preprocessing()\n",
        "\n",
        "    def load_intent_label(self):\n",
        "        f = open(self.intent_label_dir, encoding=\"UTF-8\") \n",
        "        intent_label = json.loads(f.read())\n",
        "        self.intents = list(intent_label.keys())\n",
        "        return intent_label\n",
        "    \n",
        "    def tokenize(self, sentence):\n",
        "        return sentence.split()\n",
        "\n",
        "    def tokenize_dataset(self, dataset):\n",
        "        token_dataset = []\n",
        "        for data in dataset:\n",
        "            token_dataset.append(self.tokenize(data))\n",
        "        return token_dataset\n",
        "\n",
        "    def make_intent_dataset(self, embed): # intent 분류를 위한 Dataset 생성\n",
        "        intent_dataset = pd.read_csv(self.intent_data_dir) # 데이터를 로드\n",
        "\n",
        "        labels = [self.intent_label[label] for label in intent_dataset[\"label\"].to_list()] # label\n",
        "        \n",
        "        # 사용자 발화 : 사용자가 챗봇과 커뮤니케이션하기 위해 내뱉는 말 또는 텍스트등을 의미\n",
        "        intent_querys = self.tokenize_dataset(intent_dataset[\"question\"].tolist()) # 사용자 발화 tokenize\n",
        "        \n",
        "        dataset = list(zip(intent_querys, labels)) # (사용자 발화, intent) 형태로 가공\n",
        "        intent_train_dataset, intent_test_dataset = self.word2idx_dataset(dataset, embed) # word2index\n",
        "        return intent_train_dataset, intent_test_dataset\n",
        "\n",
        "    def word2idx_dataset(self, dataset ,embed, train_ratio = 0.8):\n",
        "        embed_dataset = []\n",
        "        question_list, label_list = [], []\n",
        "        flag = True\n",
        "        random.shuffle(dataset) # 훈련용과 검증용으올 나눌 때 intent 편형이 나타나지 않도록 데이터를 셔플\n",
        "        for query, label in dataset :\n",
        "            q_vec = embed.query2idx(query) # 사용자 발화  index 형태로 변환\n",
        "            q_vec = self.prep.pad_idx_sequencing(q_vec) # 사용자 발화 최대길이 까지 padding\n",
        "\n",
        "            question_list.append(torch.tensor([q_vec]))\n",
        "            label_list.append(torch.tensor([label]))\n",
        "\n",
        "        x = torch.cat(question_list)\n",
        "        y = torch.cat(label_list)\n",
        "\n",
        "        # 학습용과 검증용으로 나누기\n",
        "        x_len = x.size()[0]\n",
        "        y_len = y.size()[0]\n",
        "        if(x_len == y_len):\n",
        "            train_size = int(x_len*train_ratio)\n",
        "            \n",
        "            \n",
        "            train_x = x[:train_size]\n",
        "            train_y = y[:train_size]\n",
        "\n",
        "            test_x = x[train_size+1:]\n",
        "            test_y = y[train_size+1:]\n",
        "            \n",
        "            # TensorDataset으로 감싸기\n",
        "            # 파이토치의 TensorDataset은 tensor를 감싸는 Dataset\n",
        "            # 인덱싱 방식과 길이를 정의함으로써 tensor의 첫 번째 차원을 따라 반복, 인덱스, 슬라이스를 위한 방법을 제공하게 됨\n",
        "            # 학습할 때 동일한 라인에서 독립 변수와 종속 변수에 쉽게 접근할 수 있음\n",
        "            train_dataset = TensorDataset(train_x,train_y)\n",
        "            test_dataset = TensorDataset(test_x,test_y)\n",
        "            \n",
        "            return train_dataset, test_dataset\n",
        "            \n",
        "        else:\n",
        "            print(\"ERROR x!=y\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural \n",
        "### tensorflow code : https://github.com/SeonbeomKim/TensorFlow-TextCNN/blob/master/TextCNN.py\n"
      ],
      "metadata": {
        "id": "zY7BO0XeWhb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nn.Conv2d(in_channels, out_channels, kernel_size)\n"
      ],
      "metadata": {
        "id": "j8gSdGEJW1x6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class textCNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, w2v, dim, kernels, dropout, num_class):\n",
        "        super(textCNN, self).__init__()\n",
        "        vocab_size = w2v.size()[0]\n",
        "        emb_dim = w2v.size()[1]\n",
        "        self.embed = nn.Embedding(vocab_size+2, emb_dim)\n",
        "        self.embed.weight[2:].data.copy_(w2v)\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, dim, (w, emb_dim)) for w in kernels])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(len(kernels)*dim, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "            emb_x = self.embed(x)\n",
        "            emb_x = emb_x.unsqueeze(1)\n",
        "\n",
        "            con_x = [conv(emb_x) for conv in self.convs]\n",
        "            # [(out_channels, conv결과 길이), ...]\n",
        "\n",
        "            pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x] \n",
        "            # [(256, 1), ...]\n",
        "\n",
        "            fc_x = torch.cat(pool_x, dim=1) # concat 하여 fcl Layer의 입력 형태로 만듬\n",
        "\n",
        "            fc_x = fc_x.squeeze(-1) # 차원 맞추기\n",
        "            fc_x = self.dropout(fc_x)\n",
        "            logit = self.fc(fc_x)\n",
        "            return logit\n",
        "\n",
        "# 모델의 가중치 저장을 위한 코드\n",
        "def save(model, save_dir, save_prefix, epoch):\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    save_prefix = os.path.join(save_dir, save_prefix)\n",
        "    save_path = '{}_steps_{}.pt'.format(save_prefix, epoch)\n",
        "    torch.save(model.state_dict(), save_path)\n"
      ],
      "metadata": {
        "id": "rk0dJxDVWf6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = embed.word2vec.wv.vectors\n",
        "weights = torch.FloatTensor(weights)\n",
        "\n",
        "num_class = len(dataset.intent_label) \n",
        "model = textCNN(weights, 256, [3,4,5], 0.5, num_class)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "4qiBQWmjXe1R",
        "outputId": "443aa628-c698-4a74-df0d-dad58850b154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-69756e299cc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintent_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'embed' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "_1bBqs5mXlcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "epoch = 10\n",
        "prev_acc = 0\n",
        "save_dir = \"./data/pretraining/1_intent_clsf_model/\"\n",
        "save_prefix = \"intent_clsf\"\n",
        "for i in range(epoch):\n",
        "    steps = 0\n",
        "    model.train() \n",
        "    #for data in train_dataloader:\n",
        "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
        "        for data in tepoch:\n",
        "            tepoch.set_description(f\"Epoch {i}\")\n",
        "            x = data[0]\n",
        "            target = data[1]\n",
        "            logit = model.forward(x)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss = F.cross_entropy(logit, target) # loass function\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            accuracy = 100.0 * corrects/x.size()[0]\n",
        "            tepoch.set_postfix(loss=loss.item(), accuracy= accuracy.numpy())\n",
        "\n",
        "    model.eval() # weight 업데이트 금지\n",
        "    steps = 0\n",
        "    accuarcy_list = []\n",
        "    #for data in test_dataloader:\n",
        "    with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
        "        for data in tepoch:\n",
        "            tepoch.set_description(f\"Epoch {i}\")\n",
        "            x = data[0]\n",
        "            target = data[1]\n",
        "\n",
        "            logit = model.forward(x)\n",
        "            loss = F.cross_entropy(logit, target)\n",
        "            corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            accuracy = 100.0 * corrects/x.size()[0]\n",
        "            accuarcy_list.append(accuracy.tolist())\n",
        "            \n",
        "            tepoch.set_postfix(loss=loss.item(), accuracy= sum(accuarcy_list)/len(accuarcy_list))\n",
        "            \n",
        "    acc = sum(accuarcy_list)/len(accuarcy_list)\n",
        "    if(acc>prev_acc):\n",
        "        prev_acc = acc\n",
        "        save(model, save_dir, save_prefix+\"_\"+str(round(acc,3)), i)\n"
      ],
      "metadata": {
        "id": "VY6ikTUzYJ4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load & Test"
      ],
      "metadata": {
        "id": "0pyXpx69YZ7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"./data/pretraining/save/1_intent_clsf_model/intent_clsf_97.217_steps_33.pt\"))\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "1MiTNk_2YcHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "q = \"제주도 오늘 날씨 알려줘\"\n",
        "\n",
        "x = dataset.prep.pad_idx_sequencing(embed.query2idx(dataset.tokenize(q)))\n",
        "\n",
        "x = torch.tensor(x)\n",
        "f = model(x.unsqueeze(0))\n",
        "\n",
        "intent = dataset.intents[torch.argmax(f).tolist()]\n",
        "\n",
        "print(\"발화 : \" + q)\n",
        "print(\"의도 : \" + intent)\n"
      ],
      "metadata": {
        "id": "yAFd4-3QZZmo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "22_intent_clsf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}